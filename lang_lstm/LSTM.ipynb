{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = namedtuple('args', ['batch_size', 'wordcount', 'epochs', 'cuda', 'embedding_dim', 'hidden_dim', 'clip', 'num_layers'])(120, 7614, 10, True, 100, 1000, 0.25, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.wordcount = args.wordcount\n",
    "        self.embedding = nn.Embedding(num_embeddings=args.wordcount, embedding_dim=args.embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-0.8, 0.8)\n",
    "        self.lstm1 = nn.LSTM(input_size=args.embedding_dim, num_layers=args.num_layers, hidden_size=args.hidden_dim, batch_first=True)\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                init.uniform(param, -0.8, 0.8)\n",
    "        self.projection = nn.Linear(in_features=args.embedding_dim, out_features=args.wordcount)\n",
    "        self.projection.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, input, lengths , stochastic=False):\n",
    "        embeddings = self.embedding(input)\n",
    "        packed = pack_padded_sequence(embeddings, lengths)\n",
    "        h_t, c_t = self.lstm1(packed)\n",
    "        output, _ = pad_packed_sequence(h_t)\n",
    "        output = self.projection(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
